{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import metrics.fundamental_analysis as fa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import importlib\n",
    "from dataclasses import dataclass, asdict\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S&P 500 tickers\n",
    "sp500_tickers = np.array(pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')[0].loc[:, 'Symbol'].tolist())\n",
    "# nasdaq_composite = np.array(pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')[0].loc[:, 'Symbol'].tolist())\n",
    "print(len(sp500_tickers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = yf.Ticker(sp500_tickers[0])\n",
    "print(t1.financials.loc['Net Income'])\n",
    "print(t1.balance_sheet.loc['Total Assets'])\n",
    "print((t1.financials.loc['Net Income'] / t1.balance_sheet.loc['Total Assets']).tolist())\n",
    "print(t1.financials.loc['Net Income'].tolist())\n",
    "print(t1.balance_sheet.loc['Total Assets'].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(fa)\n",
    "metrics = []\n",
    "for sptick in sp500_tickers:\n",
    "    try:\n",
    "        metrics.append(fa.get_fundamental_analysis_metrics(sptick))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(f'Failed to get metrics for {sptick}')\n",
    "        continue\n",
    "sp500df = pd.DataFrame.from_records([asdict(s) for s in metrics])\n",
    "sp500df.head()\n",
    "sp500df.to_csv('sp500.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y_delta in range(4):\n",
    "    roi = 'return_on_investments_y' + str(y_delta + 1)\n",
    "    related_metrics = [roi, 'return_on_assets_y' + str(y_delta + 1), 'return_on_equity_y' + str(y_delta + 1)]\n",
    "    related_metrics_corr_df = sp500df[related_metrics].corr()\n",
    "    num_related = len(related_metrics) - 1\n",
    "    # tested in cli, works\n",
    "    sp500df.loc[sp500df[roi] == -1, roi] = sum([related_metrics_corr_df.loc[roi, m] * sp500df.loc[sp500df[roi] == -1, m] for m in related_metrics[1:]]) / num_related\n",
    "\n",
    "sp500df.to_csv('s_and_p500df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_columns = len(sp500df.columns)\n",
    "clean_df = sp500df[sp500df[sp500df == -1].count(axis=1) < num_columns / 4].reset_index()\n",
    "clean_df.to_csv('clean_sp500.csv')\n",
    "very_clean_df = sp500df[sp500df[sp500df == -1].count(axis=1) == 0].reset_index()\n",
    "very_clean_df.to_csv('very_clean_sp500.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl  = yf.Ticker('AAPL')\n",
    "mmm = yf.Ticker('MMM')\n",
    "amzn = yf.Ticker('AMZN')\n",
    "print(\"========AAPL========\")\n",
    "aapl_keys = list(aapl.balance_sheet['2022-09-30'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"========MMM========\")\n",
    "mmm_keys = mmm.balance_sheet['2022-12-31'].keys().tolist()\n",
    "print(len(mmm_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"======AMZN======\")\n",
    "amzn_keys = list(amzn.balance_sheet['2022-12-31'].keys())\n",
    "print(len(amzn_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(mmm_keys) & set(amzn_keys) & set(aapl_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bal_keys = []\n",
    "for ticker in sp500_tickers[:100]:\n",
    "    tick = yf.Ticker(ticker)\n",
    "    try:\n",
    "        current_balance_sheet = tick.balance_sheet[tick.balance_sheet.keys()[0]].keys()\n",
    "    except Exception as e:\n",
    "        continue\n",
    "    if len(bal_keys) == 0:\n",
    "        bal_keys = current_balance_sheet\n",
    "    else:\n",
    "        bal_keys = list(set(bal_keys) & set(current_balance_sheet))\n",
    "pprint(bal_keys)\n",
    "# See common keys inside list of lists bal_keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "financials_keys = []\n",
    "for ticker in sp500_tickers[:100]:\n",
    "    tick = yf.Ticker(ticker)\n",
    "    try:\n",
    "        current_financial = tick.financials[tick.financials.keys()[0]].keys()\n",
    "    except Exception as e:\n",
    "        continue\n",
    "    if len(financials_keys) == 0:\n",
    "        financials_keys = current_financial\n",
    "    else:\n",
    "        financials_keys = list(set(financials_keys) & set(current_financial))\n",
    "pprint(financials_keys)\n",
    "# See common keys inside list of lists bal_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_stmt_keys = []\n",
    "for ticker in sp500_tickers[:100]:\n",
    "    tick = yf.Ticker(ticker)\n",
    "    try:\n",
    "        current_income_stmt = tick.income_stmt[tick.income_stmt.keys()[0]].keys()\n",
    "    except Exception as e:\n",
    "        continue\n",
    "    if len(financials_keys) == 0:\n",
    "        income_stmt_keys = current_income_stmt\n",
    "    else:\n",
    "        income_stmt_keys = list(set(income_stmt_keys) & set(current_income_stmt))\n",
    "pprint(financials_keys)\n",
    "# See common keys inside list of lists bal_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('clean_sp500.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.isna().sum()\n",
    "count_minus_ones = df.apply(lambda col: (col == -1).sum())\n",
    "\n",
    "# Create a new DataFrame with the count of -1 values for each column\n",
    "count_df = pd.DataFrame({'Column': count_minus_ones.index, 'Count of -1': count_minus_ones.values})\n",
    "count_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['activity_domain'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing values:\" + str(df['activity_domain'].isna().sum()))\n",
    "print(df['ticker'].loc[df['activity_domain'].isna()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity domain is na for:\n",
    "\n",
    "Berkeley Class B - should be \"Financial Services\" (Is there another BRK already?)\n",
    "\n",
    "Brown-Forman Corporation Class B - should be \"Consumer Cyclical\" (Is there another BF already?)\n",
    " \n",
    "Caterpillar Inc. - should be \"Industrials\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.at[64, 'activity_domain'] = \"Financial Services\"\n",
    "df.at[80, 'activity_domain'] = \"Consumer Cyclical\"\n",
    "df.at[92, 'activity_domain'] = \"Industrials\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = pd.get_dummies(df['activity_domain'], prefix='activity_domain')\n",
    "df = df.drop('activity_domain', axis=1)\n",
    "df = pd.concat([df, one_hot], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_convert = [col for col in df.columns if 'activity_domain' in col]\n",
    "for column in columns_to_convert:\n",
    "    df[column] = df[column].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Market Cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Missing values: ' + str(df['market_cap'].loc[df['market_cap'] == -1].count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Net Revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IGNORE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Net Income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Missing values: ' + str(df['net_income'].isna().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P/E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Missing values: ' + str(df['price_earnings_ratio'].loc[df['price_earnings_ratio'] == -1].count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('s_and_p500metrics.csv')\n",
    "roe = []\n",
    "\n",
    "def get_roe(ticker: yf.Ticker) -> float:\n",
    "    metrics = {}\n",
    "    fa.check_metric_exists_and_fill_out(ticker, metrics, 'return_on_equity', \n",
    "        lambda: (ticker.financials.loc['Net Income'] / ticker.balance_sheet.loc['Stockholders Equity']).tolist() + [ticker.info['returnOnEquity']], [-1])\n",
    "\n",
    "    return metrics['return_on_equity']\n",
    "\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    print(index)\n",
    "    ticker = yf.Ticker(row['ticker'])\n",
    "    roe.append(get_roe(ticker))\n",
    "df['return_on_equity'] = roe\n",
    "df.to_csv('s_and_p500_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "ticker = yf.Ticker(\"AAPL\")\n",
    "balance_sheet = ticker.balance_sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balance_sheet['2022-09-30'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "df = df.drop(columns=['return_on_investments_y1', 'return_on_investments_y2', 'return_on_investments_y3', 'return_on_investments_y4'])\n",
    "df = df[~(df == -1).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df.drop(['ticker', 'Unnamed: 0'], axis=1).corr()\n",
    "sn.heatmap(corr_matrix, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Very Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('very_clean_sp500.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = pd.get_dummies(df['activity_domain'], prefix='activity_domain')\n",
    "df = df.drop('activity_domain', axis=1)\n",
    "df = pd.concat([df, one_hot], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "METRICS_COUNT = 51\n",
    "\n",
    "class Autoencodertemp(torch.nn.Module): \n",
    "    def __init__(self, input_size, latent_repr_size) -> None:\n",
    "        super().__init__()\n",
    "        self.reduction = 2\n",
    "\n",
    "        encoder_modules = [] \n",
    "        depth = -1\n",
    "        while True:\n",
    "            depth += 1\n",
    "            i_size = input_size // (self.reduction ** depth)\n",
    "            o_size = input_size // (self.reduction ** (depth + 1))\n",
    "            if o_size <= latent_repr_size:\n",
    "                break\n",
    "            encoder_modules.append(torch.nn.Linear(i_size, o_size))\n",
    "            encoder_modules.append(torch.nn.ReLU())\n",
    "        encoder_modules.append(torch.nn.Linear(input_size // (self.reduction ** depth), latent_repr_size))\n",
    "        self.encoder = torch.nn.Sequential(*encoder_modules)\n",
    "\n",
    "        encoder_shapes = [layer.weight.shape for idx, layer in enumerate(self.encoder) if idx % 2 == 0]\n",
    "        print(encoder_shapes)\n",
    "\n",
    "        decoder_modules = [] \n",
    "        for i in range(0, len(self.encoder), 2):\n",
    "            reversed_shape = self.encoder[len(self.encoder) - i - 1].weight.shape\n",
    "            decoder_modules.append(torch.nn.Linear(reversed_shape[0], reversed_shape[1]))\n",
    "            if reversed_shape[0] == input_size:\n",
    "                break\n",
    "            decoder_modules.append(torch.nn.ReLU())\n",
    "        self.decoder = torch.nn.Sequential(*decoder_modules)\n",
    "        decoder_shapes = [layer.weight.shape for idx, layer in enumerate(self.decoder) if idx % 2 == 0]\n",
    "        print(decoder_shapes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x)\n",
    "        encoded = self.encoder(x)\n",
    "        print(encoded)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "    \n",
    "    \n",
    "    def train(self, data, loss_f, optim, n_epochs=20, batch_size=32):\n",
    "        data_loader = DataLoader(data, batch_size=batch_size, shuffle=False)\n",
    "        for epoch in range(n_epochs):\n",
    "            epoch_loss = 0\n",
    "            for batch_data, _ in data_loader:\n",
    "                batch_data.requires_grad = True\n",
    "                optim.zero_grad()\n",
    "                print(batch_data.shape)\n",
    "                reconstructed = self.forward(batch_data)\n",
    "                loss = loss_f(reconstructed, batch_data)\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "                epoch_loss += loss.item() * len(batch_data)\n",
    "            epoch_loss /= len(data.tensors[0])\n",
    "            print(f\"Epoch {epoch} loss: {epoch_loss}\")\n",
    "        print(\"Training finished\")\n",
    "\n",
    "    def encoder_pass(self, data):\n",
    "        return self.encoder(data)\n",
    "\n",
    "    def test(self, data):\n",
    "        with torch.no_grad():\n",
    "            reconstructed = self.forward(data)\n",
    "            return reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1402, -0.0558,  0.1962, -0.0318,  0.2077,  0.1507,  0.2515,  0.1689,\n",
       "          0.1741,  0.0211]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder_pass(train_data[0:1][0])\n",
    "#model.forward(train_data[0:1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([25, 51]), torch.Size([12, 25]), torch.Size([10, 12])]\n",
      "[torch.Size([12, 10]), torch.Size([25, 12]), torch.Size([51, 25])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 51])\n",
      "tensor([[-0.1851, -0.1021, -0.0736,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 6.8144,  4.0180,  4.2392,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2451, -0.4461, -0.4375,  ...,  0.0000,  0.0000,  1.0000],\n",
      "        ...,\n",
      "        [-0.2897,  0.5688,  0.5805,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0436, -0.2519, -0.2435,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0952,  0.1236,  0.1992,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       requires_grad=True)\n",
      "tensor([[-1.4019e-01, -5.5782e-02,  1.9616e-01, -3.1771e-02,  2.0774e-01,\n",
      "          1.5070e-01,  2.5149e-01,  1.6888e-01,  1.7410e-01,  2.1148e-02],\n",
      "        [-1.5108e-01,  2.3139e-01,  6.1302e-01, -5.5594e-01, -1.3151e-01,\n",
      "          2.5421e-01,  3.2023e-01,  4.5227e-01,  1.7759e-01,  5.4352e-01],\n",
      "        [-1.0638e-01, -7.8966e-02,  2.2418e-01, -5.2787e-02,  2.0488e-01,\n",
      "          1.4200e-01,  2.3455e-01,  1.5166e-01,  1.8583e-01,  3.4621e-02],\n",
      "        [-8.8359e-02, -8.7791e-02,  2.3212e-01, -4.4217e-02,  1.7159e-01,\n",
      "          1.4603e-01,  2.2745e-01,  1.5373e-01,  1.8365e-01,  4.2488e-02],\n",
      "        [-1.1501e-01, -6.9594e-02,  1.6376e-01, -2.8629e-02,  2.3372e-01,\n",
      "          1.4893e-01,  2.3351e-01,  1.1888e-01,  1.7898e-01, -3.1328e-03],\n",
      "        [-9.5853e-02, -9.6206e-02,  1.7304e-01, -9.2422e-02,  1.7807e-01,\n",
      "          1.4626e-01,  2.2970e-01,  1.5853e-01,  2.3096e-01,  2.2515e-02],\n",
      "        [-1.4525e-01, -3.7664e-02,  2.6022e-01, -2.1111e-02,  1.7521e-01,\n",
      "          1.1302e-01,  2.3761e-01,  1.7427e-01,  1.7698e-01,  5.7256e-02],\n",
      "        [-2.4919e-01, -1.8887e-02,  1.7775e-01, -3.2197e-01,  2.9376e-01,\n",
      "          1.7852e-01,  1.4357e-01, -6.4981e-02,  2.4931e-01,  7.6689e-02],\n",
      "        [-9.4821e-02, -8.1839e-02,  2.0941e-01, -3.8346e-02,  2.1827e-01,\n",
      "          1.5725e-01,  2.5093e-01,  1.5041e-01,  1.9190e-01,  2.4245e-02],\n",
      "        [-2.2276e-01,  6.1156e-02,  3.7567e-01,  1.2781e-02,  7.0089e-02,\n",
      "          1.0423e-01,  1.8919e-01,  1.9323e-01,  3.5416e-03,  1.9370e-01],\n",
      "        [-2.1725e-01, -2.0256e-02,  3.4746e-01,  3.4981e-03,  1.7841e-01,\n",
      "          1.4995e-01,  2.1739e-01,  1.3335e-01,  1.5686e-01,  1.3250e-01],\n",
      "        [-1.1160e-01, -7.4990e-02,  1.9049e-01, -3.2199e-02,  2.3556e-01,\n",
      "          1.6118e-01,  2.6765e-01,  1.6491e-01,  2.0477e-01,  1.5530e-02],\n",
      "        [-8.5207e-02, -8.9065e-02,  1.6195e-01, -4.9482e-02,  2.5452e-01,\n",
      "          1.7449e-01,  2.5426e-01,  1.1506e-01,  1.8679e-01, -1.1798e-02],\n",
      "        [-4.4580e-02, -1.3133e-01,  1.0379e-01, -1.0446e-01,  2.4202e-01,\n",
      "          1.9509e-01,  2.2059e-01,  8.4428e-02,  1.7041e-01, -2.2608e-02],\n",
      "        [-1.0038e-01, -8.2548e-02,  1.6777e-01, -5.2330e-02,  2.4208e-01,\n",
      "          1.6878e-01,  2.5034e-01,  1.2722e-01,  1.8436e-01, -4.2518e-03],\n",
      "        [-1.2204e-01, -4.1486e-02,  2.3868e-01, -9.7509e-03,  1.4231e-01,\n",
      "          1.3013e-01,  2.1002e-01,  1.6612e-01,  1.0083e-01,  7.0580e-02],\n",
      "        [-1.0113e-01, -7.2250e-02,  1.7800e-01, -2.5702e-02,  2.6219e-01,\n",
      "          1.4384e-01,  2.5578e-01,  1.2885e-01,  1.7428e-01,  1.2040e-03],\n",
      "        [-1.0824e-01, -6.0796e-02,  2.4175e-01, -1.2056e-02,  2.2002e-01,\n",
      "          1.5137e-01,  2.6310e-01,  1.6629e-01,  1.7905e-01,  3.9923e-02],\n",
      "        [-1.2596e+00,  1.0400e+00,  8.6236e-01,  2.0470e-01, -4.0801e-01,\n",
      "          5.1601e-02,  1.7322e-01,  1.0124e+00, -8.1642e-01,  9.8709e-01],\n",
      "        [-1.0827e-01, -7.2548e-02,  2.0421e-01, -2.6959e-02,  1.8893e-01,\n",
      "          1.5138e-01,  2.3645e-01,  1.5445e-01,  1.7652e-01,  2.9238e-02],\n",
      "        [-7.2125e-02, -1.2314e-01,  1.8297e-01, -9.7424e-02,  1.7779e-01,\n",
      "          1.6364e-01,  2.1675e-01,  1.4586e-01,  2.1495e-01,  2.7046e-02],\n",
      "        [-1.6338e-01, -4.7608e-02,  1.3938e-01, -4.4829e-02,  2.6047e-01,\n",
      "          1.3101e-01,  2.4287e-01,  1.1973e-01,  1.6921e-01, -3.2232e-02],\n",
      "        [-1.2030e-01, -6.3520e-02,  2.0437e-01, -2.3147e-02,  2.5313e-01,\n",
      "          1.5497e-01,  2.7419e-01,  1.5885e-01,  2.0394e-01,  1.2112e-02],\n",
      "        [-1.0527e-01, -7.6421e-02,  2.0160e-01, -5.3761e-02,  2.1381e-01,\n",
      "          1.6684e-01,  2.4967e-01,  1.7230e-01,  1.6649e-01,  4.3689e-02],\n",
      "        [-1.2919e-01, -5.4421e-02,  2.4349e-01, -1.5795e-02,  1.9794e-01,\n",
      "          1.4342e-01,  2.6074e-01,  2.1225e-01,  1.6673e-01,  7.1654e-02],\n",
      "        [-8.0288e-02, -1.0866e-01,  1.6282e-01, -7.2313e-02,  2.0490e-01,\n",
      "          1.7191e-01,  2.3579e-01,  1.3525e-01,  2.0792e-01,  3.7695e-03],\n",
      "        [-4.8504e-02, -1.1382e-01,  2.5597e-01, -3.3211e-02,  1.8688e-01,\n",
      "          1.7636e-01,  2.1787e-01,  1.0320e-01,  1.7152e-01,  5.6647e-02],\n",
      "        [        nan,         nan,         nan,         nan,         nan,\n",
      "                 nan,         nan,         nan,         nan,         nan],\n",
      "        [-1.3649e-01, -4.3347e-02,  2.3391e-01, -2.8898e-02,  1.9177e-01,\n",
      "          1.4959e-01,  2.3906e-01,  1.8308e-01,  1.1417e-01,  6.4198e-02],\n",
      "        [-1.1355e-01, -8.8521e-02,  2.9542e-01, -5.7817e-02,  1.7418e-01,\n",
      "          1.2350e-01,  2.2021e-01,  1.5607e-01,  2.4138e-01,  8.6050e-02],\n",
      "        [        nan,         nan,         nan,         nan,         nan,\n",
      "                 nan,         nan,         nan,         nan,         nan],\n",
      "        [-1.2377e-01, -5.9841e-02,  2.1894e-01, -9.7416e-03,  2.4932e-01,\n",
      "          1.4136e-01,  2.7786e-01,  1.7077e-01,  2.1306e-01,  2.0271e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[85], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m lr \u001b[39m=\u001b[39m \u001b[39m0.001\u001b[39m\n\u001b[0;32m     12\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mlr)\n\u001b[1;32m---> 14\u001b[0m model\u001b[39m.\u001b[39;49mtrain(train_data, loss, optimizer)\n",
      "Cell \u001b[1;32mIn[84], line 61\u001b[0m, in \u001b[0;36mAutoencodertemp.train\u001b[1;34m(self, data, loss_f, optim, n_epochs, batch_size)\u001b[0m\n\u001b[0;32m     59\u001b[0m reconstructed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(batch_data)\n\u001b[0;32m     60\u001b[0m loss \u001b[39m=\u001b[39m loss_f(reconstructed, batch_data)\n\u001b[1;32m---> 61\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     62\u001b[0m optim\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     63\u001b[0m epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem() \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(batch_data)\n",
      "Cell \u001b[1;32mIn[84], line 61\u001b[0m, in \u001b[0;36mAutoencodertemp.train\u001b[1;34m(self, data, loss_f, optim, n_epochs, batch_size)\u001b[0m\n\u001b[0;32m     59\u001b[0m reconstructed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(batch_data)\n\u001b[0;32m     60\u001b[0m loss \u001b[39m=\u001b[39m loss_f(reconstructed, batch_data)\n\u001b[1;32m---> 61\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     62\u001b[0m optim\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     63\u001b[0m epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem() \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(batch_data)\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1395\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1344\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Hp\\miniconda3\\envs\\stonks\\Lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[0;32m   2067\u001b[0m             from_this_thread\u001b[39m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[0;32m   2069\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_threads_suspended_single_notification\u001b[39m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[1;32m-> 2070\u001b[0m         keep_suspended \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n\u001b[0;32m   2072\u001b[0m frames_list \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   2074\u001b[0m \u001b[39mif\u001b[39;00m keep_suspended:\n\u001b[0;32m   2075\u001b[0m     \u001b[39m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Hp\\miniconda3\\envs\\stonks\\Lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[0;32m   2103\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_input_hook()\n\u001b[0;32m   2105\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_internal_commands()\n\u001b[1;32m-> 2106\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39m0.01\u001b[39m)\n\u001b[0;32m   2108\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[39mstr\u001b[39m(\u001b[39mid\u001b[39m(frame)))\n\u001b[0;32m   2110\u001b[0m \u001b[39m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "latent_dim = 10\n",
    "model = Autoencodertemp(len(df.columns), latent_dim)\n",
    "\n",
    "train_data = torch.tensor(train_df.values, dtype=torch.float32)\n",
    "train_data = data_utils.TensorDataset(train_data, train_data)\n",
    "loss = nn.MSELoss()\n",
    "lr = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "model.train(train_data, loss, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stonks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
