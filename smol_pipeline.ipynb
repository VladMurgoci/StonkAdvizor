{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sp500-expanded.csv')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Unnamed: 0', 'ticker'], inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "one_hot = pd.get_dummies(df['activity_domain'], prefix='activity_domain')\n",
    "df = df.drop('activity_domain', axis=1)\n",
    "df = pd.concat([df, one_hot], axis=1)\n",
    "columns_to_convert = [col for col in df.columns if 'activity_domain' in col]\n",
    "normalization_columns = ['market_cap',\n",
    "    'net_revenue_y1', 'net_revenue_y2',\n",
    "    'net_revenue_y3',\n",
    "    'net_revenue_y4', \n",
    "    'net_income_y1', \n",
    "    'net_income_y2',\n",
    "    'net_income_y3',\n",
    "    'net_income_y4',\n",
    "    'free_cash_flow',\n",
    "    'stock_price']\n",
    "for column in columns_to_convert:\n",
    "    df[column] = df[column].astype(int)\n",
    "# Large cap = stocks with 10B$ or higher market cap\n",
    "df = df[df['market_cap'] > pow(10, 9)]\n",
    "for column in df.columns:\n",
    "    if column not in columns_to_convert:\n",
    "        # if column in normalization_columns:\n",
    "        #     df[column] = (df[column] - df[column].mean()) / df[column].std()\n",
    "        # else:\n",
    "        df[column] = (df[column] - df[column].min()) / (df[column].max() - df[column].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOOK AT DATA DIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_column_distributions(df):\n",
    "    \"\"\"\n",
    "    Plot the distribution of each column in a dataframe.\n",
    "\n",
    "    :param df: Input pandas DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    # Number of columns in the dataframe\n",
    "    num_cols = df.shape[1]\n",
    "\n",
    "    # Setting up the subplots\n",
    "    fig, axes = plt.subplots(nrows=num_cols, figsize=(8, 4*num_cols))\n",
    "\n",
    "    # Iterating through each column to plot\n",
    "    for ax, (col_name, col_data) in zip(axes, df.items()):\n",
    "        if df[col_name].dtype.kind in 'bifc':  # Numerical types\n",
    "            col_data.hist(ax=ax, bins=50, edgecolor='black')\n",
    "            ax.set_title(f\"Distribution of {col_name}\")\n",
    "            ax.set_xlabel(col_name)\n",
    "            ax.set_ylabel('Frequency')\n",
    "        else:\n",
    "            # If non-numeric, we consider it categorical and use a bar plot\n",
    "            col_data.value_counts().plot(kind='bar', ax=ax)\n",
    "            ax.set_title(f\"Distribution of {col_name}\")\n",
    "            ax.set_xlabel(col_name)\n",
    "            ax.set_ylabel('Count')\n",
    "        ax.set_xlim(left=-1, right=1)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_column_distributions(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "METRICS_COUNT = 51\n",
    "\n",
    "class Autoencodertemp(torch.nn.Module): \n",
    "    def __init__(self, input_size, latent_repr_size) -> None:\n",
    "        super().__init__()\n",
    "        self.reduction = 2\n",
    "\n",
    "        encoder_modules = [] \n",
    "        depth = -1\n",
    "        while True:\n",
    "            depth += 1\n",
    "            i_size = input_size // (self.reduction ** depth)\n",
    "            o_size = input_size // (self.reduction ** (depth + 1))\n",
    "            if o_size <= latent_repr_size:\n",
    "                break\n",
    "            encoder_modules.append(torch.nn.Linear(i_size, o_size))\n",
    "            encoder_modules.append(torch.nn.ReLU())\n",
    "        encoder_modules.append(torch.nn.Linear(input_size // (self.reduction ** depth), latent_repr_size))\n",
    "        self.encoder = torch.nn.Sequential(*encoder_modules)\n",
    "\n",
    "        encoder_shapes = [layer.weight.shape for idx, layer in enumerate(self.encoder) if idx % 2 == 0]\n",
    "        print(encoder_shapes)\n",
    "\n",
    "        decoder_modules = [] \n",
    "        for i in range(0, len(self.encoder), 2):\n",
    "            reversed_shape = self.encoder[len(self.encoder) - i - 1].weight.shape\n",
    "            decoder_modules.append(torch.nn.Linear(reversed_shape[0], reversed_shape[1]))\n",
    "            if reversed_shape[0] == input_size:\n",
    "                break\n",
    "            decoder_modules.append(torch.nn.ReLU())\n",
    "        self.decoder = torch.nn.Sequential(*decoder_modules)\n",
    "        decoder_shapes = [layer.weight.shape for idx, layer in enumerate(self.decoder) if idx % 2 == 0]\n",
    "        print(decoder_shapes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "    \n",
    "    \n",
    "    def train(self, data, loss_f, optim, n_epochs=20, batch_size=32):\n",
    "        data_loader = DataLoader(data, batch_size=batch_size, shuffle=False)\n",
    "        for epoch in range(n_epochs):\n",
    "            epoch_loss = 0\n",
    "            for batch_data, _ in data_loader:\n",
    "                optim.zero_grad()\n",
    "                reconstructed = self.forward(batch_data)\n",
    "                loss = loss_f(reconstructed, batch_data)\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "                epoch_loss += loss.item() * len(batch_data)\n",
    "            epoch_loss /= len(data.tensors[0])\n",
    "            print(f\"Epoch {epoch} loss: {epoch_loss}\")\n",
    "        print(\"Training finished\")\n",
    "\n",
    "    def encoder_pass(self, data):\n",
    "        return self.encoder(data)\n",
    "\n",
    "    def test(self, data):\n",
    "        with torch.no_grad():\n",
    "            reconstructed = self.forward(data)\n",
    "            return reconstructed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data for training/testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.utils.data as data_utils\n",
    "from hyperopt import hp, fmin, tpe, space_eval, Trials\n",
    "\n",
    "def hyper_tune(params):\n",
    "    model = Autoencodertemp(len(df.columns), params['latent_dim'])\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n",
    "    loss_f = params['loss']\n",
    "    data_loader = DataLoader(train_data, batch_size=32, shuffle=False)\n",
    "    for epoch in range(params['epochs']):\n",
    "        epoch_loss = 0\n",
    "        for batch_data, _ in data_loader:\n",
    "            optim.zero_grad()\n",
    "            reconstructed = model(batch_data)\n",
    "            loss = loss_f(reconstructed, batch_data)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            epoch_loss += loss.item() * len(batch_data)\n",
    "        epoch_loss /= len(train_data.tensors[0])\n",
    "        print(f\"Epoch {epoch} loss: {epoch_loss}\")\n",
    "    print(\"Training finished\")\n",
    "    return epoch_loss\n",
    "\n",
    "space = {\n",
    "    \"lr\": hp.loguniform(\"lr\", -5, 0),\n",
    "    \"latent_dim\": hp.choice(\"latent_dim\", [2, 3, 4, 5, 6, 7, 8, 9, 10]),\n",
    "    \"loss\": hp.choice(\"loss\", [nn.MSELoss(), nn.L1Loss()]),\n",
    "    \"weight_decay\": hp.loguniform(\"weight_decay\", -8, -4),\n",
    "    \"epochs\": hp.choice(\"epochs\", range(1, 16)),\n",
    "}\n",
    "\n",
    "train_data = torch.tensor(train_df.values, dtype=torch.float32)\n",
    "train_data = data_utils.TensorDataset(train_data, train_data)\n",
    "best = fmin(fn=hyper_tune, space=space, algo=tpe.suggest, max_evals=100, trials=Trials())\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stonks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
